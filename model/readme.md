## CPP .gguf quantized LLMs
* This folder contains the .gguf CPP quantized models for local/offline inference.
* The.gguf files are available at [HuggingFace] (https://huggingface.co/models?filter=cpp-quantized-llms)
* For swapping and combining, in the seed-model.sh file, change the `MODEL_ID` variable in the to the model you want to use.
* - url is currently set to Hermes-3-Llama-3.1-8B-GGUF/resolve/main/${model}